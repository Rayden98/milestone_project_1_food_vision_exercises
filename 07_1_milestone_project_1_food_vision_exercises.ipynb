{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Get helper functions"
      ],
      "metadata": {
        "id": "x3GtaFruCDT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download helper functions script\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "metadata": {
        "id": "m39cGLysCMdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import series of helper functions for the notebook\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"
      ],
      "metadata": {
        "id": "iXeB3RmFCN-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Use TensorFlow Datasets to Download Data\n",
        "If you want to get an overview of TensorFlow Datasets (TFDS), read the guide: https://www.tensorflow.org/datasets/overview"
      ],
      "metadata": {
        "id": "kLTGJj9-CPU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get TensorFlow Datasets\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "8VTxceiWCSkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all available datasets\n",
        "datasets_list = tfds.list_builders()  # get all available datasets in TFDS\n",
        "print(\"food101\" in datasets_list) # is our target dataset in the list of TFDS datasets?"
      ],
      "metadata": {
        "id": "lFN-TvGvCT-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the data (takes 5-6 minutes in Google Colab)\n",
        "(train_data, test_data), ds_info = tfds.load(name=\"food101\",\n",
        "                                             split=[\"train\", \"validation\"],\n",
        "                                             shuffle_files=True,\n",
        "                                             as_supervised=True, #data gets returned in tuple format (data, label)\n",
        "                                             with_info=True)"
      ],
      "metadata": {
        "id": "N0EkPLOnCVLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Batch & prepare datasets\n",
        "We're now going to make our data input pipeline run really fast.\n",
        "\n",
        "For more resources on this, I'd highly going through the following guide: https://www.tensorflow.org/guide/data_performance"
      ],
      "metadata": {
        "id": "7zD9-sgiCWt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map preprocessing function to training (and parallelize)\n",
        "train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# Shuffle train_data and turn it into batches and prefetch it (load it faster)\n",
        "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Map preprocessing function to test data\n",
        "test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "sARc_nkmCkw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data"
      ],
      "metadata": {
        "id": "GeEf4IgbCmSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create modelling callbacks\n",
        "We're going to create a couple of callbacks to help us while our model trains:\n",
        "\n",
        "TensorBoard callback to log training results (so we can visulize them later if need be)\n",
        "ModelCheckpoint callback to save our model's progress after feature extraction."
      ],
      "metadata": {
        "id": "CqKwMsUKCoPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tensorboard callback (import from helper_functions.py)\n",
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "# Create ModelCheckpoint callback to save a model's progress during training\n",
        "checkpoint_path = \"model_checkpoints/cp.ckpt\"\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      monitor=\"val_acc\",\n",
        "                                                      save_best_only=True,\n",
        "                                                      save_weights_only=True,\n",
        "                                                      verbose=0) # don't print whether or not model is being saved"
      ],
      "metadata": {
        "id": "wgPiJLU5Cq6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup mixed precision training\n",
        "First and foremost, for a deeper understanding of mixed precision training, check out the TensorFlow guide for mixed precision: https://www.tensorflow.org/guide/mixed_precision\n",
        "\n",
        "Mixed precision utilizes a combination of float32 and float16 data types to speed up model performance."
      ],
      "metadata": {
        "id": "eSzy8-AtCtcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn on mixed precision training\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")  # set global data policy to mixed precision"
      ],
      "metadata": {
        "id": "CQx5ucdMCxG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "UJOq0X40CyyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mixed_precision.global_policy()"
      ],
      "metadata": {
        "id": "Xi5pSaGRCzsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build feature extraction model"
      ],
      "metadata": {
        "id": "WzBZzkVmC1HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "# Create base model\n",
        "input_shape = (224, 224, 3)\n",
        "base_model = tf.keras.applications.EfficientNetB4(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create functional model\n",
        "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "# Note: EfficientNetBX models have rescaling built-in but if your model doesn't you can have a layer like below\n",
        "# X = preprocessing.Rescaling(1./255)(x)\n",
        "x = base_model(inputs, training=False)    # make sure layers which should be inference mode only stay like that\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(len(class_names))(x)\n",
        "outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "3XVqBpxkC5Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label"
      ],
      "metadata": {
        "id": "eWTM5lL2DJh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "gDfms0SDDK8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Checking layer dtype policies (are we using mixed precision?)"
      ],
      "metadata": {
        "id": "PikPHxXGDL3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the dtype_policy attributes of layers in our model\n",
        "for layer in model.layers:\n",
        "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
      ],
      "metadata": {
        "id": "dy-5V6FVDOae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the dtype policy attributes of layers attributes of layer in the base model\n",
        "for layer in model.layers[1].layers: # check the layers of the base model (layer at index 1 of `model`)\n",
        "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
      ],
      "metadata": {
        "id": "XfXvLCbYDP_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mixed_precision.global_policy()"
      ],
      "metadata": {
        "id": "91Ww1zb5DRmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fit the feature extraction model\n",
        "If our goal is to fine-tune a pretrained model, the general order of doing things is:\n",
        "\n",
        "Build a feature extraction model (train a couple output layers with base layers frozen)\n",
        "Fine-tune some of frozen layers"
      ],
      "metadata": {
        "id": "uUvFwGTyDUhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the feature extraction model with callbacks\n",
        "history_101_food_classes_feature_extract = model.fit(train_data,\n",
        "                                                     epochs=3,\n",
        "                                                     steps_per_epoch=(len(train_data)),\n",
        "                                                     validation_data=test_data,\n",
        "                                                     validation_steps=int(0.15 * len(test_data)),\n",
        "                                                     callbacks=[create_tensorboard_callback(dir_name=\"training_logs\",\n",
        "                                                                                            experiment_name=\"efficientnetb0_101_classes_all_data_feature_extract\"),\n",
        "                                                                model_checkpoint])"
      ],
      "metadata": {
        "id": "c1HURbIGDX0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze all of the layer in the base model\n",
        "model.trainable = True"
      ],
      "metadata": {
        "id": "HDrLvZzOLGVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "  print(layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "xVROkIA1LuJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to see what dtype_policy of the layers in your loaded model are\n",
        "for layer in  model.layers: # check the layers of the base model (layer at index 1 of `model`)\n",
        "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
      ],
      "metadata": {
        "id": "alaPAkL7L092"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "# Monitor the val_loss and stop training if it doesn't improve for 3 epochs\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping for more\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                  patience=3)\n",
        "\n",
        "\n",
        "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
        "# Save the best model only\n",
        "# Monitor val_loss while training and save the best model (lowest val_loss)\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint for more\n",
        "checkpoint_filepath = \"model_checkpoints2/cp.ckpt\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                               monitor='val_loss',\n",
        "                                                               save_best_only=True)"
      ],
      "metadata": {
        "id": "XbWM6BJWL5JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating learning rate reduction callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, # print out when learning rate goes down\n",
        "                                                 min_lr=1e-7)"
      ],
      "metadata": {
        "id": "zKvBAJiAL8VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model ready for fine-tuning\n",
        "# Use the Adam optimizer with a 10x lower than default learning rate\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # learning rate lower by 10x\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "uMXdGKYRL9l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start to fine-tune (all layers)\n",
        "# Use 100 epochs as the default\n",
        "# Validate on 15% of the test_data\n",
        "# Use the create_tensorboard_callback, ModelCheckpoint and EarlyStopping callbacks you created eaelier\n",
        "# YOUR_CODE_HERE\n",
        "history_101_food_classes_feature_extract = model.fit(train_data,\n",
        "                                                     epochs=100, # fine-tune for a maximum of 100 epochs\n",
        "                                                     steps_per_epoch=(len(train_data)),\n",
        "                                                     validation_data=test_data,\n",
        "                                                     validation_steps=int(0.15 * len(test_data)),\n",
        "                                                     callbacks=[create_tensorboard_callback(dir_name=\"training_logs\",\n",
        "                                                                                            experiment_name=\"efficientnetb0_101_classes_all_data_feature_extract\"),\n",
        "                                                                model_checkpoint_callback,\n",
        "                                                                early_stopping,\n",
        "                                                                reduce_lr])"
      ],
      "metadata": {
        "id": "RN1iKrFWMAI4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}